{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a721c9",
   "metadata": {},
   "source": [
    "# MultiTask Face Analysis Model Evaluation Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8420c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, eval, matplotlib.pyplot as plt, os\n",
    "import torch, torchvision, datasets\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "from multitask.models import MultiTaskFaceAnalysisModel\n",
    "from configs.train_multitask import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa944f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1mv2 = datasets.MS1MV2()\n",
    "num_classes = ms1mv2.number_of_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0fd028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/models/davit_t_adaface_MS1MV2_Dataset/backbone.pth\n",
      "Loaded pretrained backbone from data/models/davit_t_adaface_MS1MV2_Dataset/backbone.pth.\n",
      "Loaded pretrained face recognition subnet from data/models/davit_t_adaface_MS1MV2_Dataset/recognition_subnet.pth\n",
      "\n",
      "AdaFace with the following property\n",
      "self.m 0.4\n",
      "self.h 0.333\n",
      "self.s 64\n",
      "self.t_alpha 0.99\n"
     ]
    }
   ],
   "source": [
    "model = MultiTaskFaceAnalysisModel(num_classes = num_classes, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee90611c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(os.path.join('data', 'models', 'multitask_davit_t_face_emotion_age_gender_race', 'model.pth'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea4f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_recognition_model = nn.Sequential(\n",
    "    model.backbone,\n",
    "    model.face_recognition_embedding_subnet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e47f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: LFW...\n",
      "Finished processing LFW.\n",
      "Processing dataset: CPLFW...\n",
      "Finished processing CPLFW.\n",
      "Processing dataset: CALFW...\n",
      "Finished processing CALFW.\n",
      "Processing dataset: CFP-FP...\n",
      "Finished processing CFP-FP.\n",
      "Processing dataset: CFP-FF...\n",
      "Finished processing CFP-FF.\n",
      "Processing dataset: AgeDB30...\n",
      "Finished processing AgeDB30.\n",
      "Processing dataset: VGG2FP...\n",
      "Finished processing VGG2FP.\n"
     ]
    }
   ],
   "source": [
    "metrics = eval.evaluate_face_recognition(face_recognition_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c803e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LFW = 0.9978333333333167.\n",
      "F1 score for LFW = 0.9978257853705955\n",
      "Accuracy for CPLFW = 0.9279999999999846.\n",
      "F1 score for CPLFW = 0.9250478445250513\n",
      "Accuracy for CALFW = 0.9584999999999839.\n",
      "F1 score for CALFW = 0.9568860490254536\n",
      "Accuracy for CFP-FP = 0.959714285714272.\n",
      "F1 score for CFP-FP = 0.9583161672353014\n",
      "Accuracy for CFP-FF = 0.9978571428571286.\n",
      "F1 score for CFP-FF = 0.9978524170558153\n",
      "Accuracy for AgeDB30 = 0.9763333333333171.\n",
      "F1 score for AgeDB30 = 0.975873615691968\n",
      "Accuracy for VGG2FP = 0.951599999999981.\n",
      "F1 score for VGG2FP = 0.9500311586498981\n"
     ]
    }
   ],
   "source": [
    "for key, db_metrics in metrics.items():\n",
    "    accuracy, _, _, f1_score, _, _, _, _ = db_metrics\n",
    "    print(f'Accuracy for {key} = {accuracy}.')\n",
    "    print(f'F1 score for {key} = {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f9f5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskFaceAnalysisModel(\n",
       "  (backbone): DaViT(\n",
       "    (model): DaViT(\n",
       "      (patch_embeds): ModuleList(\n",
       "        (0): PatchEmbed(\n",
       "          (proj): Conv2d(3, 96, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PatchEmbed(\n",
       "          (proj): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PatchEmbed(\n",
       "          (proj): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): PatchEmbed(\n",
       "          (proj): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (main_blocks): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): MySequential(\n",
       "            (0): SpatialBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): MySequential(\n",
       "            (0): SpatialBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.018)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.027)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): MySequential(\n",
       "            (0): SpatialBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.036)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.045)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): MySequential(\n",
       "            (0): SpatialBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.055)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.064)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): MySequential(\n",
       "            (0): SpatialBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.073)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.082)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): MySequential(\n",
       "            (0): SpatialBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.091)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelBlock(\n",
       "              (cpe): ModuleList(\n",
       "                (0-1): 2 x ConvPosEnc(\n",
       "                  (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (face_recognition_embedding_subnet): FaceRecognitionEmbeddingSubnet(\n",
       "    (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (feature_head): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (1): BatchNorm1d(768, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Linear(in_features=768, out_features=512, bias=False)\n",
       "      (3): BatchNorm1d(512, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (margin_head): AdaFace()\n",
       "  (emotion_recognition_subnet): EmotionRecognitionSubnet(\n",
       "    (fusion): MultiScaleFusion(\n",
       "      (conv11): Conv2d(96, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv12): Conv2d(47, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv13): Conv2d(47, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv21): Conv2d(192, 93, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv22): Conv2d(93, 93, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv31): Conv2d(384, 186, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv41): Conv2d(768, 186, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (cbam): CBAM(\n",
       "      (sam): SAM(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (cam): CAM(\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=64, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (3): BatchNorm1d(256, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU()\n",
       "      (5): Linear(in_features=256, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (age_estimation_subnet): AgeEstimationSubnet(\n",
       "    (fusion): MultiScaleFusion(\n",
       "      (conv11): Conv2d(96, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv12): Conv2d(47, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv13): Conv2d(47, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv21): Conv2d(192, 93, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv22): Conv2d(93, 93, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv31): Conv2d(384, 186, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv41): Conv2d(768, 186, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (cbam): CBAM(\n",
       "      (sam): SAM(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (cam): CAM(\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=64, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (3): BatchNorm1d(256, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=256, out_features=102, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (gender_recognition_subnet): GenderRecognitionSubnet(\n",
       "    (fusion): MultiScaleFusion(\n",
       "      (conv11): Conv2d(96, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv12): Conv2d(47, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv13): Conv2d(47, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv21): Conv2d(192, 93, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv22): Conv2d(93, 93, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv31): Conv2d(384, 186, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv41): Conv2d(768, 186, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (cbam): CBAM(\n",
       "      (sam): SAM(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (cam): CAM(\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=64, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (3): BatchNorm1d(256, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU()\n",
       "      (5): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (race_recognition_subnet): RaceRecognitionSubnet(\n",
       "    (fusion): MultiScaleFusion(\n",
       "      (conv11): Conv2d(96, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv12): Conv2d(47, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv13): Conv2d(47, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv21): Conv2d(192, 93, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv22): Conv2d(93, 93, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv31): Conv2d(384, 186, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (conv41): Conv2d(768, 186, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (cbam): CBAM(\n",
       "      (sam): SAM(\n",
       "        (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (cam): CAM(\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=64, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (3): BatchNorm1d(256, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): ReLU()\n",
       "      (5): Linear(in_features=256, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce3610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = v2.Compose([ # for testing on datasets other than face recognition.\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale = True),\n",
    "    v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd89cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rafdb_test = datasets.RAFDB(subset = 'test', transform = test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12e57af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raf_test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset = rafdb_test, \n",
    "    batch_size = 64,\n",
    "    num_workers = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15e8ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "emotion_recognition_accuracy, loss = eval.evaluate_emotion(model = model, dataloader = raf_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7892546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888526727509778"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_recognition_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0a0411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "utk_face_db = datasets.UTKFace(subset = 'test', transform = test_transform)\n",
    "utk_face_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset = utk_face_db,\n",
    "    batch_size = 64,\n",
    "    num_workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6487645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "age_mae = eval.evaluate_age(model, utk_face_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fea456f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.375353720179083"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9857155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "gender_accuracy, loss = eval.evaluate_gender(model, utk_face_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34aadbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9636977627691009"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b2ee719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    }
   ],
   "source": [
    "race_accuracy, loss = eval.evalate_race(model, utk_face_dataloader, device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "169489d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.879273955255382"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59954ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
